# BayesianOptimization.jl å®˜æ–¹APIä½¿ç”¨æŒ‡å—

## ğŸ“– æ¦‚è¿°

BayesianOptimization.jl æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´å¶æ–¯ä¼˜åŒ–åº“ï¼ŒåŸºäºé«˜æ–¯è¿‡ç¨‹çš„ä»£ç†æ¨¡å‹æ¥ä¼˜åŒ–æ˜‚è´µçš„é»‘ç®±å‡½æ•°ã€‚

## ğŸš€ æ ¸å¿ƒç»„ä»¶

### 1. ä¸»è¦APIç»“æ„

```julia
# åŸºæœ¬å·¥ä½œæµ
opt = BOpt(func, model, acquisition, modeloptimizer, lowerbounds, upperbounds; kwargs...)
result = boptimize!(opt)
```

### 2. ä¸»è¦å¯¼å‡ºç±»å‹å’Œå‡½æ•°

**æ ¸å¿ƒå‡½æ•°ï¼š**
- `BOpt` - è´å¶æ–¯ä¼˜åŒ–å™¨æ„é€ å‡½æ•°
- `boptimize!` - æ‰§è¡Œä¼˜åŒ–
- `optimize` - å¿«é€Ÿä¼˜åŒ–æ¥å£ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰

**é‡‡é›†å‡½æ•°ï¼š**
- `ExpectedImprovement()` - æœŸæœ›æ”¹è¿›
- `ProbabilityOfImprovement()` - æ”¹è¿›æ¦‚ç‡
- `UpperConfidenceBound()` - ä¸Šç½®ä¿¡ç•Œ
- `ThompsonSamplingSimple()` - ç®€å•æ±¤æ™®æ£®é‡‡æ ·
- `MutualInformation()` - äº’ä¿¡æ¯

**æ¨¡å‹ä¼˜åŒ–å™¨ï¼š**
- `MAPGPOptimizer` - æœ€å¤§åéªŒä¼°è®¡ä¼˜åŒ–å™¨
- `NoModelOptimizer` - ä¸ä¼˜åŒ–æ¨¡å‹è¶…å‚æ•°

**å…¶ä»–å·¥å…·ï¼š**
- `ScaledSobolIterator`, `ScaledLHSIterator` - åˆå§‹åŒ–é‡‡æ ·
- `Min`, `Max` - ä¼˜åŒ–æ–¹å‘
- `Silent`, `Timings`, `Progress` - è¯¦ç»†ç¨‹åº¦
- `maxduration!`, `maxiterations!` - åŠ¨æ€è°ƒæ•´

## ğŸ¯ å‘ç›®æ ‡å‡½æ•°ä¼ é€’é¢å¤–å‚æ•°

BayesianOptimization.jl æ”¯æŒå¤šç§æ–¹å¼å‘ç›®æ ‡å‡½æ•°ä¼ é€’é¢å¤–å‚æ•°ï¼š

### æ–¹æ³•1ï¼šä½¿ç”¨é—­åŒ… (æ¨è)

```julia
# é—®é¢˜å‚æ•°
param1 = 2.0
param2 = [1.0, 2.0, 3.0]

# åˆ›å»ºé—­åŒ…å‡½æ•°
function create_objective(p1, p2)
    return function objective(x)
        # ä½¿ç”¨å¤–éƒ¨å‚æ•° p1, p2 å’Œè¾“å…¥ x
        return -(x[1] - p1)^2 - (x[2] - p1)^2 + sum(p2)
    end
end

# ç”Ÿæˆå¸¦å‚æ•°çš„ç›®æ ‡å‡½æ•°
f_with_params = create_objective(param1, param2)

# ç”¨äºä¼˜åŒ–
opt = BOpt(f_with_params, model, acquisition, modeloptimizer, 
           lowerbounds, upperbounds)
```

### æ–¹æ³•2ï¼šä½¿ç”¨ lambda è¡¨è¾¾å¼

```julia
# å‚æ•°
noise_level = 0.1
scale_factor = 2.0

# åˆ›å»ºå¸¦å‚æ•°çš„ç›®æ ‡å‡½æ•°
f = x -> expensive_function(x, noise_level, scale_factor)

opt = BOpt(f, model, acquisition, modeloptimizer, bounds...)
```

### æ–¹æ³•3ï¼šä½¿ç”¨æŸ¯é‡ŒåŒ–ï¼ˆCurryingï¼‰

```julia
# åŸå§‹å‡½æ•°æ¥å—å¤šä¸ªå‚æ•°
function original_function(x, param1, param2, param3)
    return sum(x.^2) + param1 * sum(x) + param2 * prod(x) + param3
end

# æŸ¯é‡ŒåŒ–ï¼šå›ºå®šæŸäº›å‚æ•°
curried_f = x -> original_function(x, 1.5, 2.0, -0.5)

opt = BOpt(curried_f, model, acquisition, modeloptimizer, bounds...)
```

### æ–¹æ³•4ï¼šä½¿ç”¨å‡½æ•°å¯¹è±¡ï¼ˆFunctorï¼‰

```julia
# å®šä¹‰åŒ…å«å‚æ•°çš„ç»“æ„ä½“
struct ParametricObjective{T}
    param1::T
    param2::T
    param3::Vector{T}
end

# è®©ç»“æ„ä½“å¯è°ƒç”¨
function (obj::ParametricObjective)(x)
    return -(x[1] - obj.param1)^2 - (x[2] - obj.param2)^2 + 
           sum(obj.param3 .* x)
end

# åˆ›å»ºç›®æ ‡å‡½æ•°å¯¹è±¡
params = ParametricObjective(1.0, 2.0, [0.5, 0.8])

opt = BOpt(params, model, acquisition, modeloptimizer, bounds...)
```

### æ–¹æ³•5ï¼šä½¿ç”¨å…¨å±€å˜é‡ï¼ˆä¸æ¨èï¼Œä½†å¯è¡Œï¼‰

```julia
# å…¨å±€å‚æ•°ï¼ˆé¿å…åœ¨å¹¶è¡Œç¯å¢ƒä¸­ä½¿ç”¨ï¼‰
global PARAM1 = 1.5
global PARAM2 = [1, 2, 3]

function objective_with_globals(x)
    return -(x[1] - PARAM1)^2 + sum(PARAM2 .* x)
end

opt = BOpt(objective_with_globals, model, acquisition, modeloptimizer, bounds...)
```

### å®é™…åº”ç”¨ç¤ºä¾‹ï¼šç‰©ç†æ¨¡æ‹Ÿå‚æ•°ä¼˜åŒ–

```julia
using BayesianOptimization, GaussianProcesses

# ç‰©ç†å‚æ•°
struct PhysicsParams
    temperature::Float64
    pressure::Float64
    material_constants::Vector{Float64}
end

# æ˜‚è´µçš„ç‰©ç†æ¨¡æ‹Ÿå‡½æ•°
function physics_simulation(design_params, physics_params::PhysicsParams)
    # æ¨¡æ‹Ÿå¤æ‚çš„ç‰©ç†è¿‡ç¨‹
    x1, x2 = design_params[1], design_params[2]
    T, P = physics_params.temperature, physics_params.pressure
    
    # ç¤ºä¾‹ï¼šæŸç§ç‰©ç†é‡çš„è®¡ç®—
    result = exp(-x1^2/T) * cos(x2*P) + sum(physics_params.material_constants .* design_params)
    
    # æ·»åŠ è®¡ç®—å»¶è¿Ÿæ¨¡æ‹Ÿæ˜‚è´µå‡½æ•°
    sleep(0.01)  # æ¨¡æ‹Ÿ100msçš„è®¡ç®—æ—¶é—´
    
    return result
end

# è®¾ç½®ç‰©ç†ç¯å¢ƒå‚æ•°
physics_params = PhysicsParams(300.0, 1.0, [0.5, 0.3])

# åˆ›å»ºå¸¦å‚æ•°çš„ç›®æ ‡å‡½æ•°
objective = x -> physics_simulation(x, physics_params)

# åˆ›å»ºä¼˜åŒ–å™¨
model = ElasticGPE(2, mean = MeanConst(0.0), 
                   kernel = SEArd([1.0, 1.0], 0.0))

opt = BOpt(objective,
           model,
           ExpectedImprovement(),
           MAPGPOptimizer(every = 5, 
                         noisebounds = [-4, 3],
                         kernbounds = [[-2, -2, -3], [3, 3, 2]]),
           [-3.0, -3.0], [3.0, 3.0],
           maxiterations = 20,
           sense = Max)

# æ‰§è¡Œä¼˜åŒ–
result = boptimize!(opt)
println("æœ€ä¼˜è®¾è®¡å‚æ•°: ", result.observed_optimizer)
println("åœ¨ç»™å®šç‰©ç†æ¡ä»¶ä¸‹çš„æœ€ä¼˜å€¼: ", result.observed_optimum)
```

### æ³¨æ„äº‹é¡¹

1. **æ€§èƒ½**: é—­åŒ…æ–¹æ³•é€šå¸¸æ€§èƒ½æœ€å¥½ï¼Œå› ä¸ºå‚æ•°åœ¨ç¼–è¯‘æ—¶è¢«å†…è”
2. **å†…å­˜**: é—­åŒ…ä¼šæ•è·å¤–éƒ¨å˜é‡ï¼Œæ³¨æ„å†…å­˜ä½¿ç”¨
3. **ç±»å‹ç¨³å®šæ€§**: ç¡®ä¿é—­åŒ…å†…çš„å‚æ•°ç±»å‹æ˜¯ç¨³å®šçš„
4. **å¹¶è¡Œæ€§**: é¿å…ä½¿ç”¨å…¨å±€å˜é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­
5. **è°ƒè¯•**: é—­åŒ…å¯èƒ½ä½¿è°ƒè¯•æ›´å›°éš¾ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨å‡½æ•°å¯¹è±¡

## ğŸ”§ è¯¦ç»†ç”¨æ³•

### BOpt æ„é€ å‡½æ•°

```julia
BOpt(func, model, acquisition, modeloptimizer, lowerbounds, upperbounds;
     sense = Max,                    # ä¼˜åŒ–æ–¹å‘ (Max/Min)
     maxiterations = 10^4,           # æœ€å¤§è¿­ä»£æ¬¡æ•°
     maxduration = Inf,              # æœ€å¤§è¿è¡Œæ—¶é—´ (ç§’)
     acquisitionoptions = NamedTuple(), # é‡‡é›†å‡½æ•°ä¼˜åŒ–é€‰é¡¹
     repetitions = 1,                # æ¯ä¸ªç‚¹çš„é‡å¤è¯„ä¼°æ¬¡æ•°
     verbosity = Progress,           # è¾“å‡ºè¯¦ç»†ç¨‹åº¦ (Silent/Timings/Progress)
     initializer_iterations = 5*length(lowerbounds), # åˆå§‹é‡‡æ ·ç‚¹æ•°
     initializer = ScaledSobolIterator(lowerbounds, upperbounds, initializer_iterations))  # åˆå§‹åŒ–é‡‡æ ·å™¨
```

**é‡è¦å‚æ•°è¯´æ˜ï¼š**
- `func`: ç›®æ ‡å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªå‘é‡å‚æ•° `x`ï¼Œè¿”å›æ ‡é‡å€¼
- `model`: é«˜æ–¯è¿‡ç¨‹æ¨¡å‹ (å¦‚ `ElasticGPE` æˆ–é¢„åŠ è½½çš„ `GP`)
- `acquisition`: é‡‡é›†å‡½æ•° (å¦‚ `ExpectedImprovement()`)
- `modeloptimizer`: æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–å™¨ (å¦‚ `MAPGPOptimizer`)
- `lowerbounds/upperbounds`: æœç´¢ç©ºé—´è¾¹ç•Œå‘é‡

### MAPGPOptimizer è¯¦ç»†é…ç½®

```julia
# å…³é”®ï¼škernbounds çš„æ­£ç¡®æ ¼å¼
MAPGPOptimizer(
    every = 20,                     # æ¯20æ­¥ä¼˜åŒ–ä¸€æ¬¡è¶…å‚æ•°
    noisebounds = [-4, 3],          # å¯¹æ•°å™ªå£°è¾¹ç•Œ [ä¸‹ç•Œ, ä¸Šç•Œ]
    kernbounds = [
        [-3*ones(d); -3],           # ä¸‹ç•Œï¼š[æ ¸å‚æ•°ä¸‹ç•Œ..., å¯¹æ•°ä¿¡å·æ–¹å·®ä¸‹ç•Œ]
        [4*ones(d); 3]              # ä¸Šç•Œï¼š[æ ¸å‚æ•°ä¸Šç•Œ..., å¯¹æ•°ä¿¡å·æ–¹å·®ä¸Šç•Œ]
    ],
    maxeval = 100                   # è¶…å‚æ•°ä¼˜åŒ–çš„æœ€å¤§è¯„ä¼°æ¬¡æ•°
)
```

**é‡è¦è¯´æ˜ï¼š**
- `kernbounds` æ˜¯ `[ä¸‹ç•Œå‘é‡, ä¸Šç•Œå‘é‡]` æ ¼å¼
- å¯¹äº `SEArd` æ ¸ï¼šéœ€è¦ `d+1` ä¸ªå‚æ•°ï¼ˆdä¸ªé•¿åº¦å°ºåº¦ + 1ä¸ªä¿¡å·æ–¹å·®ï¼‰
- è¾¹ç•Œå¿…é¡»æ»¡è¶³ `ä¸‹ç•Œ[i] <= ä¸Šç•Œ[i]`

### é«˜æ–¯è¿‡ç¨‹æ¨¡å‹è®¾ç½®

```julia
using GaussianProcesses

# åˆ›å»ºå¼¹æ€§é«˜æ–¯è¿‡ç¨‹æ¨¡å‹
model = ElasticGPE(
    d,                              # è¾“å…¥ç»´åº¦
    mean = MeanConst(0.0),         # å‡å€¼å‡½æ•°
    kernel = SEArd(zeros(d), 0.0), # æ ¸å‡½æ•°ï¼šè‡ªåŠ¨ç›¸å…³ç¡®å®šçš„å¹³æ–¹æŒ‡æ•°æ ¸
    logNoise = -2.0,               # å¯¹æ•°å™ªå£°
    capacity = 3000                # å®¹é‡
)

# é¢„åŠ è½½æ•°æ®çš„æƒ…å†µ
X_init = # dÃ—n çŸ©é˜µï¼ˆç‰¹å¾ç»´åº¦Ã—æ ·æœ¬æ•°ï¼‰
y_init = # nç»´å‘é‡
gp = GP(X_init, y_init, MeanConst(0.0), SEArd(ones(d), 0.0))
```

## ğŸ“‹ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŸºæœ¬ç”¨æ³•

```julia
using BayesianOptimization, GaussianProcesses

# ç›®æ ‡å‡½æ•°
f(x) = -(x[1] - 2)^2 - (x[2] + 1)^2 + 5

# æ¨¡å‹
model = ElasticGPE(2, mean = MeanConst(0.0), 
                   kernel = SEArd([0.0, 0.0], 0.0), 
                   logNoise = -2.0)

# æ¨¡å‹ä¼˜åŒ–å™¨ - æ­£ç¡®çš„kernboundsæ ¼å¼
modeloptimizer = MAPGPOptimizer(
    every = 10,
    noisebounds = [-4, 3],
    kernbounds = [[-2, -2, -3], [3, 3, 2]], # [x1_scale, x2_scale, signal_var]
    maxeval = 100
)

# ä¼˜åŒ–å™¨
opt = BOpt(f, model, ExpectedImprovement(), modeloptimizer,
           [-5.0, -5.0], [5.0, 5.0],
           sense = Max,
           maxiterations = 50,
           verbosity = Progress)

# æ‰§è¡Œä¼˜åŒ–
result = boptimize!(opt)
println("æœ€ä¼˜è§£: ", result.observed_optimizer)
println("æœ€ä¼˜å€¼: ", result.observed_optimum)
```

### ç¤ºä¾‹2ï¼šé¢„åŠ è½½æ•°æ®çš„çƒ­å¯åŠ¨

```julia
# å·²æœ‰çš„æ•°æ®ç‚¹
X_init = [[-1.0, 1.0], [0.0, 0.0], [2.0, -1.0]]  # åˆå§‹ç‚¹åˆ—è¡¨
y_init = [f(x) for x in X_init]                   # å¯¹åº”çš„å‡½æ•°å€¼

# è½¬æ¢ä¸ºGPéœ€è¦çš„æ ¼å¼
X_matrix = hcat(X_init...)  # 2Ã—3 çŸ©é˜µ
y_vector = Vector{Float64}(y_init)

# åˆ›å»ºé¢„åŠ è½½çš„GP
gp = GP(X_matrix, y_vector, MeanConst(0.0), SEArd([1.0, 1.0], 0.0))

# ä¼˜åŒ–å™¨ï¼ˆè®¾ç½® initializer_iterations = 0ï¼‰
opt = BOpt(f, gp, UpperConfidenceBound(), 
           NoModelOptimizer(),  # ä½¿ç”¨å›ºå®šè¶…å‚æ•°
           [-5.0, -5.0], [5.0, 5.0],
           sense = Max,
           maxiterations = 30,
           initializer_iterations = 0,  # ä¸è¿›è¡Œé¢å¤–åˆå§‹åŒ–
           verbosity = Progress)

result = boptimize!(opt)
```

### ç¤ºä¾‹3ï¼šé‡‡é›†å‡½æ•°æ¯”è¾ƒ

```julia
# ä¸€ç»´å‡½æ•°
f_1d(x) = -(x[1] - 3)^2 + 5

# ä¸€ç»´åˆå§‹æ•°æ®
X_1d = reshape([1.0, 2.0, 4.0], 1, 3)  # 1Ã—3 çŸ©é˜µ
y_1d = [f_1d([x]) for x in [1.0, 2.0, 4.0]]

# ä¸€ç»´GP
gp_1d = GP(X_1d, y_1d, MeanConst(0.0), SEArd([1.0], 0.0))

# ä¸€ç»´ä¼˜åŒ–å™¨
opt_1d = BOpt(f_1d, gp_1d, ExpectedImprovement(),
              MAPGPOptimizer(every = 5, 
                           noisebounds = [-4, 3],
                           kernbounds = [[-2, -3], [3, 2]]), # [length_scale, signal_var]
              [0.0], [6.0],
              sense = Max,
              maxiterations = 20,
              initializer_iterations = 0)

result_1d = boptimize!(opt_1d)
```

## âš™ï¸ é«˜çº§é…ç½®

### é‡‡é›†å‡½æ•°é€‰é¡¹

```julia
# é‡‡é›†å‡½æ•°ä¼˜åŒ–é…ç½®
acquisitionoptions = (
    method = :LD_LBFGS,    # NLoptä¼˜åŒ–æ–¹æ³•
    restarts = 10,         # éšæœºé‡å¯æ¬¡æ•°
    maxeval = 2000,        # æœ€å¤§è¯„ä¼°æ¬¡æ•°
    maxtime = 0.1          # æœ€å¤§æ—¶é—´é™åˆ¶
)

opt = BOpt(f, model, acquisition, modeloptimizer, bounds...,
           acquisitionoptions = acquisitionoptions)
```

### ä¸åŒé‡‡é›†å‡½æ•°çš„ç‰¹ç‚¹

```julia
# æœŸæœ›æ”¹è¿›ï¼ˆæœ€å¸¸ç”¨ï¼‰
ExpectedImprovement()

# ä¸Šç½®ä¿¡ç•Œï¼ˆé€‚åˆæ¢ç´¢ï¼‰
UpperConfidenceBound(scaling = BrochuBetaScaling(0.1), Î²t = 1.0)

# æ”¹è¿›æ¦‚ç‡ï¼ˆä¿å®ˆï¼‰
ProbabilityOfImprovement()

# æ±¤æ™®æ£®é‡‡æ ·ï¼ˆéšæœºï¼‰
ThompsonSamplingSimple()

# äº’ä¿¡æ¯ï¼ˆç†è®ºæœ€ä¼˜ï¼‰
MutualInformation()
```

## ğŸ” å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. kernbounds è¾¹ç•Œé”™è¯¯
**é”™è¯¯**: `ArgumentError("invalid NLopt arguments: bounds 1 fail -1 <= 1 <= -2")`

**è§£å†³**: ç¡®ä¿ä¸‹ç•Œ <= ä¸Šç•Œ
```julia
# é”™è¯¯
kernbounds = [[-1, 3], [-2, 2]]  # -2 < 3 è¿åäº†ç¬¬äºŒä¸ªå‚æ•°çš„è¾¹ç•Œ

# æ­£ç¡®
kernbounds = [[-2, -3], [3, 2]]  # æ‰€æœ‰ä¸‹ç•Œéƒ½å°äºå¯¹åº”ä¸Šç•Œ
```

### 2. GP æ•°æ®æ ¼å¼
```julia
# GPéœ€è¦çš„æ•°æ®æ ¼å¼
X = hcat(points...)     # dÃ—n çŸ©é˜µï¼ˆç‰¹å¾ç»´åº¦Ã—æ ·æœ¬æ•°ï¼‰
y = Vector{Float64}(values)  # nç»´å‘é‡

# ä¸æ˜¯ nÃ—d çŸ©é˜µï¼
```

### 3. é¿å…è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜
```julia
# å¦‚æœMAPGPOptimizeræœ‰é—®é¢˜ï¼Œä½¿ç”¨å›ºå®šå‚æ•°
opt = BOpt(f, gp, acquisition, NoModelOptimizer(), bounds...)
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [GitHub ä»“åº“](https://github.com/jbrea/BayesianOptimization.jl)
- [å®˜æ–¹æ–‡æ¡£](https://jbrea.github.io/BayesianOptimization.jl/dev/)
- Brochu et al. (2010): "A Tutorial on Bayesian Optimization"

## ï¿½ APIæ–‡æ¡£æ£€æŸ¥ä¸éªŒè¯æŠ¥å‘Š

### âœ… å·²éªŒè¯çš„APIè¦ç‚¹

1. **BOptæ„é€ å‡½æ•°ç­¾å**: ä¸æºç å®Œå…¨ä¸€è‡´
2. **MAPGPOptimizerå‚æ•°**: kernboundsæ ¼å¼æ­£ç¡® `[[ä¸‹ç•Œ...], [ä¸Šç•Œ...]]`
3. **é«˜æ–¯è¿‡ç¨‹æ•°æ®æ ¼å¼**: ç¡®è®¤éœ€è¦ dÃ—n çŸ©é˜µæ ¼å¼
4. **é‡‡é›†å‡½æ•°ç±»å‹**: æ‰€æœ‰åˆ—å‡ºçš„é‡‡é›†å‡½æ•°éƒ½å­˜åœ¨ä¸”æ­£ç¡®
5. **ç›®æ ‡å‡½æ•°æ”¯æŒ**: å®Œå…¨æ”¯æŒé—­åŒ…ã€lambdaå’Œå‡½æ•°å¯¹è±¡ç­‰å‚æ•°ä¼ é€’æ–¹å¼

### ğŸ”§ ä¿®æ­£çš„é—®é¢˜

1. **initializerå‚æ•°**: è¡¥å……äº†å®Œæ•´çš„æ„é€ å‡½æ•°ç­¾å
2. **å‡½æ•°å‚æ•°ä¼ é€’**: æ·»åŠ äº†è¯¦ç»†çš„å¤šç§å‚æ•°ä¼ é€’æ–¹æ³•
3. **ç¤ºä¾‹ä»£ç **: ä¿®æ­£äº†å˜é‡åä¸ä¸€è‡´çš„é—®é¢˜

### ğŸ¯ é¢å¤–å‘ç°

1. **Juliaç‰ˆæœ¬å…¼å®¹æ€§**: è­¦å‘Šä¿¡æ¯æ˜¯å·²çŸ¥çš„éè‡´å‘½æ€§é—®é¢˜
2. **æ€§èƒ½ä¼˜åŒ–**: æºç æ˜¾ç¤ºæ”¯æŒ ElasticGPE ç”¨äºå¤§è§„æ¨¡ä¼˜åŒ–
3. **è°ƒè¯•æ”¯æŒ**: æä¾›äº†å¤šç§ verbosity çº§åˆ«å’Œ TimerOutput

## ï¿½ğŸ’¡ æœ€ä½³å®è·µ

1. **å¼€å§‹ç®€å•**: å…ˆç”¨ `NoModelOptimizer` æµ‹è¯•åŸºæœ¬åŠŸèƒ½
2. **æ•°æ®æ ¼å¼**: ç¡®ä¿ X æ˜¯ dÃ—n æ ¼å¼ï¼Œy æ˜¯å‘é‡
3. **è¾¹ç•Œè®¾ç½®**: ä»”ç»†æ£€æŸ¥ `kernbounds` çš„è¾¹ç•Œå…³ç³»
4. **é‡‡é›†å‡½æ•°**: `ExpectedImprovement` é€šå¸¸æ˜¯æœ€å¥½çš„èµ·ç‚¹
5. **è°ƒè¯•**: ä½¿ç”¨ `verbosity = Progress` ç›‘æ§ä¼˜åŒ–è¿‡ç¨‹

## ğŸ¯ æ€»ç»“

BayesianOptimization.jl æä¾›äº†å®Œæ•´çš„è´å¶æ–¯ä¼˜åŒ–åŠŸèƒ½ï¼Œå…³é”®æ˜¯ï¼š
- æ­£ç¡®è®¾ç½®æ•°æ®æ ¼å¼ï¼ˆdÃ—nçŸ©é˜µï¼‰
- åˆç†é…ç½® `kernbounds`ï¼ˆä¸‹ç•Œ <= ä¸Šç•Œï¼‰
- é€‰æ‹©åˆé€‚çš„é‡‡é›†å‡½æ•°å’Œæ¨¡å‹ä¼˜åŒ–å™¨
- ä»ç®€å•é…ç½®å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§


æ³¨æ„ï¼š
â”Œ Warning: use values(kwargs) and keys(kwargs) instead of kwargs.data and kwargs.itr
â”‚   caller = #MAPGPOptimizer#19 at gp.jl:37 [inlined]
å…³äºè¿™ä¸ªè­¦å‘Šçš„è¯¦ç»†è¯´æ˜ï¼š
ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªè­¦å‘Šï¼Ÿ
Julia è¯­è¨€æ¼”è¿›ï¼šJulia åœ¨ç‰ˆæœ¬æ›´æ–°ä¸­æ”¹è¿›äº†å‘½åå…ƒç»„(NamedTuple)çš„API
åŒ…çš„æ»åæ›´æ–°ï¼šBayesianOptimization.jl åŒ…è¿˜æ²¡æœ‰å®Œå…¨é€‚é…æœ€æ–°çš„Juliaè¯­æ³•
å‘åå…¼å®¹æ€§ï¼šJuliaä¿æŒäº†å‘åå…¼å®¹ï¼Œæ‰€ä»¥æ—§ä»£ç ä»èƒ½è¿è¡Œï¼Œåªæ˜¯ä¼šç»™å‡ºè­¦å‘Š